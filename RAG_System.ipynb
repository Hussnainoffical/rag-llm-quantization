{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7ZLaoCxS6hk"
      },
      "outputs": [],
      "source": [
        "\n",
        "### Key Features Implemented:\n",
        "- **Model**: `unsloth/Qwen2.5-3B-bnb-4bit` ‚Äì pre-quantized with Unsloth's optimized 4-bit (NF4) format\n",
        "- **Dynamic Quantization**: Unsloth selectively keeps higher precision for critical parameters (e.g., attention weights, outliers) while compressing others to 4 bits ‚Üí better accuracy + lower VRAM than standard bitsandbytes 4-bit\n",
        "- **Domain-specific documents**: Custom text files on Python, Machine Learning, and RAG\n",
        "- **Chunking**: 200-word chunks with 50-word overlap for better context retention\n",
        "- **Embedding**: `all-MiniLM-L6-v2` sentence transformer\n",
        "- **Vector Store**: FAISS (FlatL2 for exact search on small corpus)\n",
        "- **Retrieval**: Top-k similar chunks fetched and injected into prompt\n",
        "- **Generation**: Grounded responses using the quantized LLM\n",
        "- **Memory Optimization**: Designed to run efficiently on free Colab T4 GPU (~6-7GB VRAM peak)\n",
        "\n",
        "**Why Unsloth Dynamic 4-bit?**\n",
        "Standard 4-bit quantization (bitsandbytes) compresses all weights uniformly ‚Üí can hurt accuracy.\n",
        "Unsloth uses **dynamic scaling**: detects outlier weights and keeps them in higher precision ‚Üí near FP16 accuracy with 4-bit memory footprint.\n",
        "Result: A 3B model runs in ~5GB VRAM instead of 12+GB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3LXOSZwNb2e"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"hf_AkUD\")  # Replace with your actual token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxtXOCsWNoi1",
        "outputId": "2da49809-46e2-4324-cb64-9745e3700dd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2026.1.2: Fast Qwen2 patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model_name = \"unsloth/Qwen2.5-3B-bnb-4bit\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = 2048,  # Max input length\n",
        "    dtype = torch.float16,  # Memory-efficient data type\n",
        "    load_in_4bit = True,    # Dynamic 4-bit quantization\n",
        ")\n",
        "\n",
        "# Enable faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7xiuJ-2NbxX",
        "outputId": "e0239f9a-43b3-44e1-9c6c-db2c4b656d75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‚Äòdocs‚Äô: File exists\n",
            "Documents created! You can add more by uploading to the 'docs' folder.\n"
          ]
        }
      ],
      "source": [
        "!mkdir docs\n",
        "\n",
        "# Sample document 1: python.txt\n",
        "with open(\"docs/python.txt\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "Python is a high-level programming language known for its simplicity and readability.\n",
        "It is widely used in web development, data analysis, artificial intelligence, and automation.\n",
        "Key libraries include NumPy for numerical computations, Pandas for data manipulation, and TensorFlow for machine learning.\n",
        "Python's syntax emphasizes code readability with indentation instead of braces.\n",
        "\"\"\")\n",
        "\n",
        "# Sample document 2: machine_learning.txt\n",
        "with open(\"docs/machine_learning.txt\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "Machine learning is a subset of artificial intelligence that enables systems to learn from data.\n",
        "Common types include supervised learning (e.g., regression, classification), unsupervised learning (e.g., clustering), and reinforcement learning.\n",
        "Algorithms like decision trees, neural networks, and support vector machines are fundamental.\n",
        "Overfitting is a common issue where models perform well on training data but poorly on new data.\n",
        "\"\"\")\n",
        "\n",
        "# Sample document 3: rag.txt (about RAG itself, for testing)\n",
        "with open(\"docs/rag.txt\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "Retrieval-Augmented Generation (RAG) combines retrieval from external documents with generative AI models.\n",
        "It improves accuracy by grounding responses in real data, reducing hallucinations.\n",
        "Steps include indexing documents, embedding queries, retrieving chunks, and generating responses.\n",
        "Unsloth's 4-bit quantization makes RAG efficient on limited hardware.\n",
        "\"\"\")\n",
        "\n",
        "print(\"Documents created! You can add more by uploading to the 'docs' folder.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NktEajrNbsd",
        "outputId": "f03a7f9a-21f8-4c8d-a7c0-670b534561e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 3 chunks (with 50-word overlap) from 3 domain-specific documents.\n",
            "Domain: Artificial Intelligence, Python, and RAG systems\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "\n",
        "# Load embedding model\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Improved chunking with overlap\n",
        "def chunk_text(text, chunk_size=200, overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        chunk = words[i:i + chunk_size]\n",
        "        chunks.append(\" \".join(chunk))\n",
        "        i += (chunk_size - overlap)  # Move forward by chunk_size minus overlap\n",
        "    return chunks\n",
        "\n",
        "# Read and chunk all documents\n",
        "texts = []\n",
        "sources = []  # Track which file each chunk came from\n",
        "\n",
        "for file in os.listdir(\"docs\"):\n",
        "    path = os.path.join(\"docs\", file)\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        content = f.read()\n",
        "        chunks = chunk_text(content, chunk_size=200, overlap=50)\n",
        "        texts.extend(chunks)\n",
        "        sources.extend([file] * len(chunks))\n",
        "\n",
        "embeddings = embedder.encode(texts)\n",
        "\n",
        "print(f\"Created {len(texts)} chunks (with 50-word overlap) from {len(os.listdir('docs'))} domain-specific documents.\")\n",
        "print(\"Domain: Artificial Intelligence, Python, and RAG systems\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3GIPgr5QLUN",
        "outputId": "2b8249d3-fec6-4d30-a550-9e09dd900553"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e76G5r5aNbp1",
        "outputId": "7c43c97e-e500-48f4-a655-b19bf2bc0701"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS index built successfully with 3 chunks! üéâ\n",
            "Using exact search (FlatL2) ‚Äì perfect for small/medium document sets.\n",
            "Retrieval ready!\n"
          ]
        }
      ],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Convert embeddings to numpy float32\n",
        "embeddings = np.array(embeddings).astype('float32')\n",
        "\n",
        "# Get dimension\n",
        "dimension = embeddings.shape[1]\n",
        "\n",
        "# Use simple FlatL2 index (best for small number of documents/chunks)\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Add embeddings directly (no training needed)\n",
        "index.add(embeddings)\n",
        "\n",
        "print(f\"FAISS index built successfully with {len(texts)} chunks! üéâ\")\n",
        "print(\"Using exact search (FlatL2) ‚Äì perfect for small/medium document sets.\")\n",
        "\n",
        "# Retrieval function (unchanged)\n",
        "def retrieve_chunks(query, top_k=3):\n",
        "    query_emb = embedder.encode([query]).astype('float32')\n",
        "    distances, indices = index.search(query_emb, top_k)\n",
        "    retrieved = [texts[i] for i in indices[0]]\n",
        "    return retrieved\n",
        "\n",
        "print(\"Retrieval ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyoJ-Y3nNbnK",
        "outputId": "474bcba4-68cd-4fbc-eee3-1da309ec5e6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full RAG pipeline ready!\n"
          ]
        }
      ],
      "source": [
        "def rag_generate(query, top_k=3, max_tokens=200):\n",
        "    # Retrieve and display chunks\n",
        "    chunks = retrieve_and_show(query, top_k)\n",
        "    context = \"\\n\\n\".join(chunks)\n",
        "\n",
        "    prompt = f\"\"\"Use ONLY the following context to answer the question. Be accurate, concise, and professional.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=0.7,\n",
        "        do_sample=True\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = answer[len(prompt):].strip()  # Remove prompt echo\n",
        "\n",
        "    return answer\n",
        "\n",
        "print(\"Full RAG pipeline ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "y0teGQRrSZ7N"
      },
      "outputs": [],
      "source": [
        "# Enhanced retrieval with visibility\n",
        "def retrieve_and_show(query, top_k=3):\n",
        "    query_emb = embedder.encode([query]).astype('float32')\n",
        "    distances, indices = index.search(query_emb, top_k)\n",
        "\n",
        "    print(f\"\\nüîç Query: {query}\")\n",
        "    print(f\"üìö Retrieved {top_k} most relevant chunks:\\n\")\n",
        "    retrieved_chunks = []\n",
        "    for idx, dist in zip(indices[0], distances[0]):\n",
        "        chunk = texts[idx]\n",
        "        source = sources[idx]\n",
        "        print(f\"Source: {source} | Distance: {dist:.4f}\")\n",
        "        print(f\"Chunk: {chunk.strip()}\\n\")\n",
        "        print(\"-\" * 80)\n",
        "        retrieved_chunks.append(chunk)\n",
        "    return retrieved_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l653uBv_NbkH",
        "outputId": "ef3d9d77-9e61-4c68-caed-bdaaebe88005"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Question: What is Python used for?\n",
            "\n",
            "üîç Query: What is Python used for?\n",
            "üìö Retrieved 3 most relevant chunks:\n",
            "\n",
            "Source: python.txt | Distance: 0.3753\n",
            "Chunk: Python is a high-level programming language known for its simplicity and readability. It is widely used in web development, data analysis, artificial intelligence, and automation. Key libraries include NumPy for numerical computations, Pandas for data manipulation, and TensorFlow for machine learning. Python's syntax emphasizes code readability with indentation instead of braces.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Source: rag.txt | Distance: 1.6343\n",
            "Chunk: Retrieval-Augmented Generation (RAG) combines retrieval from external documents with generative AI models. It improves accuracy by grounding responses in real data, reducing hallucinations. Steps include indexing documents, embedding queries, retrieving chunks, and generating responses. Unsloth's 4-bit quantization makes RAG efficient on limited hardware.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Source: machine_learning.txt | Distance: 1.6735\n",
            "Chunk: Machine learning is a subset of artificial intelligence that enables systems to learn from data. Common types include supervised learning (e.g., regression, classification), unsupervised learning (e.g., clustering), and reinforcement learning. Algorithms like decision trees, neural networks, and support vector machines are fundamental. Overfitting is a common issue where models perform well on training data but poorly on new data.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Answer: Python is a high-level programming language that is used for various purposes, such as web development, data analysis, artificial intelligence, and automation.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Question: Explain machine learning in simple terms.\n",
            "\n",
            "üîç Query: Explain machine learning in simple terms.\n",
            "üìö Retrieved 3 most relevant chunks:\n",
            "\n",
            "Source: machine_learning.txt | Distance: 0.6383\n",
            "Chunk: Machine learning is a subset of artificial intelligence that enables systems to learn from data. Common types include supervised learning (e.g., regression, classification), unsupervised learning (e.g., clustering), and reinforcement learning. Algorithms like decision trees, neural networks, and support vector machines are fundamental. Overfitting is a common issue where models perform well on training data but poorly on new data.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Source: python.txt | Distance: 1.4969\n",
            "Chunk: Python is a high-level programming language known for its simplicity and readability. It is widely used in web development, data analysis, artificial intelligence, and automation. Key libraries include NumPy for numerical computations, Pandas for data manipulation, and TensorFlow for machine learning. Python's syntax emphasizes code readability with indentation instead of braces.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Source: rag.txt | Distance: 1.6003\n",
            "Chunk: Retrieval-Augmented Generation (RAG) combines retrieval from external documents with generative AI models. It improves accuracy by grounding responses in real data, reducing hallucinations. Steps include indexing documents, embedding queries, retrieving chunks, and generating responses. Unsloth's 4-bit quantization makes RAG efficient on limited hardware.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Answer: Machine learning is a field within artificial intelligence that allows systems to learn from data. It includes various types like supervised, unsupervised, and reinforcement learning, and uses algorithms such as decision trees and neural networks. The main focus is on improving accuracy and reducing errors in model performance.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Question: How does Retrieval-Augmented Generation work?\n",
            "\n",
            "üîç Query: How does Retrieval-Augmented Generation work?\n",
            "üìö Retrieved 3 most relevant chunks:\n",
            "\n",
            "Source: rag.txt | Distance: 0.6983\n",
            "Chunk: Retrieval-Augmented Generation (RAG) combines retrieval from external documents with generative AI models. It improves accuracy by grounding responses in real data, reducing hallucinations. Steps include indexing documents, embedding queries, retrieving chunks, and generating responses. Unsloth's 4-bit quantization makes RAG efficient on limited hardware.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Source: machine_learning.txt | Distance: 1.8921\n",
            "Chunk: Machine learning is a subset of artificial intelligence that enables systems to learn from data. Common types include supervised learning (e.g., regression, classification), unsupervised learning (e.g., clustering), and reinforcement learning. Algorithms like decision trees, neural networks, and support vector machines are fundamental. Overfitting is a common issue where models perform well on training data but poorly on new data.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Source: python.txt | Distance: 2.0679\n",
            "Chunk: Python is a high-level programming language known for its simplicity and readability. It is widely used in web development, data analysis, artificial intelligence, and automation. Key libraries include NumPy for numerical computations, Pandas for data manipulation, and TensorFlow for machine learning. Python's syntax emphasizes code readability with indentation instead of braces.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Answer: Retrieval-Augmented Generation (RAG) combines retrieval from external documents with generative AI models to improve accuracy and reduce hallucinations. The process involves indexing documents, embedding queries, retrieving chunks of relevant data, and generating responses. This approach ensures that responses are grounded in real data, enhancing the overall quality and reliability of the AI's output.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Question: What are key Python libraries for AI?\n",
            "\n",
            "üîç Query: What are key Python libraries for AI?\n",
            "üìö Retrieved 3 most relevant chunks:\n",
            "\n",
            "Source: python.txt | Distance: 0.6884\n",
            "Chunk: Python is a high-level programming language known for its simplicity and readability. It is widely used in web development, data analysis, artificial intelligence, and automation. Key libraries include NumPy for numerical computations, Pandas for data manipulation, and TensorFlow for machine learning. Python's syntax emphasizes code readability with indentation instead of braces.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Source: machine_learning.txt | Distance: 1.3180\n",
            "Chunk: Machine learning is a subset of artificial intelligence that enables systems to learn from data. Common types include supervised learning (e.g., regression, classification), unsupervised learning (e.g., clustering), and reinforcement learning. Algorithms like decision trees, neural networks, and support vector machines are fundamental. Overfitting is a common issue where models perform well on training data but poorly on new data.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Source: rag.txt | Distance: 1.4891\n",
            "Chunk: Retrieval-Augmented Generation (RAG) combines retrieval from external documents with generative AI models. It improves accuracy by grounding responses in real data, reducing hallucinations. Steps include indexing documents, embedding queries, retrieving chunks, and generating responses. Unsloth's 4-bit quantization makes RAG efficient on limited hardware.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Answer: The key Python libraries for AI include NumPy for numerical computations, Pandas for data manipulation, and TensorFlow for machine learning. These libraries are commonly used in various AI applications, such as natural language processing, computer vision, and robotics.\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test queries\n",
        "queries = [\n",
        "    \"What is Python used for?\",\n",
        "    \"Explain machine learning in simple terms.\",\n",
        "    \"How does Retrieval-Augmented Generation work?\",\n",
        "    \"What are key Python libraries for AI?\"\n",
        "]\n",
        "\n",
        "for q in queries:\n",
        "    print(f\"\\nQuestion: {q}\")\n",
        "    print(f\"Answer: {rag_generate(q)}\\n\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_8n4TmkSlUj",
        "outputId": "c9c3fddd-20ba-41ce-b30f-9f97716f556a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== VRAM Usage After Model Load ===\n",
            "GPU: Tesla T4\n",
            "VRAM Allocated: 4.12 GB\n",
            "VRAM Reserved:  4.27 GB\n",
            "Thu Jan  8 15:47:42 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   79C    P0             69W /   70W |    4520MiB /  15360MiB |     42%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# VRAM Monitoring\n",
        "import torch\n",
        "\n",
        "def print_vram():\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"VRAM Allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
        "        print(f\"VRAM Reserved:  {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")\n",
        "    else:\n",
        "        print(\"No GPU detected\")\n",
        "\n",
        "print(\"=== VRAM Usage After Model Load ===\")\n",
        "print_vram()\n",
        "\n",
        "# Run !nvidia-smi for full view\n",
        "!nvidia-smi"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
